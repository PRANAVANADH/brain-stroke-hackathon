{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KB6M78gJ1daa",
        "outputId": "679ce2ee-77b9-4bf4-a57f-3c93ef03eb2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stroke class distribution:\n",
            " stroke\n",
            "0    4733\n",
            "1     248\n",
            "Name: count, dtype: int64\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.6948 - loss: 0.5985 - val_accuracy: 0.7793 - val_loss: 0.5054\n",
            "Epoch 2/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7871 - loss: 0.4622 - val_accuracy: 0.7878 - val_loss: 0.4484\n",
            "Epoch 3/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7807 - loss: 0.4692 - val_accuracy: 0.7888 - val_loss: 0.4396\n",
            "Epoch 4/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7924 - loss: 0.4489 - val_accuracy: 0.7962 - val_loss: 0.4311\n",
            "Epoch 5/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7980 - loss: 0.4340 - val_accuracy: 0.8036 - val_loss: 0.4068\n",
            "Epoch 6/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8006 - loss: 0.4349 - val_accuracy: 0.8004 - val_loss: 0.4106\n",
            "Epoch 7/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8035 - loss: 0.4369 - val_accuracy: 0.8068 - val_loss: 0.4076\n",
            "Epoch 8/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8081 - loss: 0.4144 - val_accuracy: 0.8068 - val_loss: 0.3865\n",
            "Epoch 9/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8003 - loss: 0.4265 - val_accuracy: 0.8120 - val_loss: 0.4052\n",
            "Epoch 10/10\n",
            "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7988 - loss: 0.4267 - val_accuracy: 0.8247 - val_loss: 0.3686\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://94d08c5dfd05a948b3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://94d08c5dfd05a948b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import gradio as gr\n",
        "\n",
        "# Try to import google.colab; if it fails, assume we're not in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Check if files exist\n",
        "file1 = \"/content/full_data (4).csv\"\n",
        "file2 = \"/content/full_filled_stroke_data (1) (1).csv\"\n",
        "\n",
        "# If in Colab and files don't exist, prompt for upload\n",
        "if IN_COLAB and (not os.path.exists(file1) or not os.path.exists(file2)):\n",
        "    from google.colab import files\n",
        "    print(f\"Error: '{file1}' or '{file2}' not found! Please upload them.\")\n",
        "    uploaded = files.upload()\n",
        "    # Check if uploaded files match expected names\n",
        "    if file1 not in uploaded or file2 not in uploaded:\n",
        "        raise FileNotFoundError(f\"Uploaded files do not match expected names: '{file1}', '{file2}'\")\n",
        "elif not os.path.exists(file1) or not os.path.exists(file2):\n",
        "    # If not in Colab, raise the FileNotFoundError\n",
        "    raise FileNotFoundError(f\"Error: '{file1}' or '{file2}' not found! Please check file paths.\")\n",
        "\n",
        "# Load datasets\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "\n",
        "# Load datasets\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "\n",
        "# Data preprocessing\n",
        "df1 = pd.get_dummies(df1, columns=['work_type', 'smoking_status'])\n",
        "df1['gender'] = df1['gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
        "df1['ever_married'] = df1['ever_married'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "df1['Residence_type'] = df1['Residence_type'].apply(lambda x: 1 if x == 'Urban' else 0)\n",
        "\n",
        "# Features and labels\n",
        "X = df1.drop(columns='stroke')\n",
        "y = df1['stroke']\n",
        "\n",
        "# Feature scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Check class balance\n",
        "stroke_counts = y.value_counts()\n",
        "print(\"Stroke class distribution:\\n\", stroke_counts)\n",
        "\n",
        "if len(stroke_counts) < 2:\n",
        "    raise ValueError(\"The target 'stroke' needs both 0s and 1s. Found only one class.\")\n",
        "\n",
        "# Oversampling using SMOTE\n",
        "smt = SMOTE(random_state=42)\n",
        "X_smt, y_smt = smt.fit_resample(X_scaled, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_smt, y_smt, test_size=0.1, random_state=42)\n",
        "\n",
        "# Building the neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "callback = EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[callback], verbose=1)\n",
        "\n",
        "# Plot accuracy and loss\n",
        "def plot_model_performance(history):\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Predict and evaluate\n",
        "def evaluate_model():\n",
        "    y_pred = (model.predict(X_test[:500]) > 0.5).astype(\"int32\")\n",
        "    accuracy = accuracy_score(y_test[:500], y_pred) * 100\n",
        "    classification_rep = classification_report(y_test[:500], y_pred)\n",
        "\n",
        "    plot_model_performance(history)\n",
        "\n",
        "    return f\"Accuracy: {accuracy:.2f}%\\n\\nClassification Report:\\n{classification_rep}\"\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=evaluate_model,\n",
        "    inputs=[],\n",
        "    outputs=\"text\",\n",
        "    title=\"Stroke Prediction Results\",\n",
        "    description=\"Click the button below to view the stroke prediction analysis based on deep learning.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio\n",
        "interface.launch(share=True)  # For Colab, use share=True\n"
      ]
    }
  ]
}